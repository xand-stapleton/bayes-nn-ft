{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, FloatLogSlider, FloatText\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_pickles(directory):\n",
    "    hyperparam_hist = []\n",
    "    test_loss_hist = []\n",
    "    evaluation_hist = []\n",
    "    killed_param_hist = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pickle'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "\n",
    "                evaluation_hist.append(data['evaluation_hist'])\n",
    "                hyperparam_hist.append(data['hp'])\n",
    "                test_loss_hist.append(data['test_loss_hist'])\n",
    "                killed_param_hist.append(data['killed_param_hist'])\n",
    "            # From the bootstrap file\n",
    "            # pickle.dump({'hp': hp, 'test_loss_hist': test_loss_hist,\n",
    "            # 'evaluation_hist': evaluation_hist,\n",
    "            # 'killed_param_hist': killed_param_hist}, handle)\n",
    "                        \n",
    "    return hyperparam_hist, test_loss_hist, evaluation_hist, killed_param_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dir = 'output/'\n",
    "hyperparam_list, test_loss_hist, evaluation_hist, killed_param_hist = unpack_pickles(search_dir)\n",
    "# print(all_lists)\n",
    "test_loss_hist = np.array(test_loss_hist)\n",
    "\n",
    "# per_cutoff_losses is a list: [[losses_at_cutoff_i], [losses_at_cutoff_i+1], ...]\n",
    "ensemble_size = len(test_loss_hist)\n",
    "no_cutoffs = len(test_loss_hist[0])\n",
    "\n",
    "# Cutoffs:\n",
    "# IMPORTANT NOTE: I've added a 0 to the beginning here as there's one eval with no cutoff\n",
    "cutoffs = np.array([0, 1e-4, 5e-3, 7.5e-3, 1e-2, 5e-2, 7.5e-2,\n",
    "                    1e-1, 5e-1, 7.5e-1, 1, 5, 7.5, 10, 50, 75,\n",
    "                    100, 5e2, 7.5e2, 1e3, 5e3, 7.5e3, 1e4,\n",
    "                    5e4, 7.5e4, 1e5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outlier_indices(data, top_percentile=95, bottom_percentile=5):\n",
    "    \"\"\"\n",
    "    Find the indices of outliers in the top and bottom percentiles of the data, and indices of non-outliers.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): The input data.\n",
    "        top_percentile (float): The percentile above which values are considered outliers. Default is 95.\n",
    "        bottom_percentile (float): The percentile below which values are considered outliers. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three arrays:\n",
    "            - Indices of top outliers\n",
    "            - Indices of bottom outliers\n",
    "            - Indices of non-outliers\n",
    "    \"\"\"\n",
    "    top_threshold = np.percentile(data, top_percentile)\n",
    "    bottom_threshold = np.percentile(data, bottom_percentile)\n",
    "\n",
    "    top_outliers_indices = np.where(data > top_threshold)[0]\n",
    "    bottom_outliers_indices = np.where(data < bottom_threshold)[0]\n",
    "\n",
    "    all_indices = np.arange(len(data))\n",
    "    non_outliers_indices = np.setdiff1d(all_indices, np.concatenate([top_outliers_indices, bottom_outliers_indices]))\n",
    "\n",
    "    return top_outliers_indices, bottom_outliers_indices, non_outliers_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses_var_cutoff(idx, feature):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Feature is either a list of lists or a list of floats\n",
    "    plt.scatter(np.arange(len(feature[:, idx])), feature[:, idx], marker='o', color='b')\n",
    "    # plt.title(f'Losses for cutoff {cutoffs[idx]}')\n",
    "    plt.xlabel('Ensemble member')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/relu_act/act-relu_cutoff-{cutoffs[idx]}_first_ts.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ensemble size: ', ensemble_size)\n",
    "print('No cutoffs: ', no_cutoffs)\n",
    "\n",
    "print('Pre-average shape: ', test_loss_hist.shape)\n",
    "test_loss_hist_averaged = np.average(test_loss_hist, axis=2)\n",
    "print('Post-average shape: ', test_loss_hist_averaged.shape)\n",
    "mask = test_loss_hist_averaged[:, 0] < 1e3\n",
    "test_loss_hist_averaged = test_loss_hist_averaged[mask]\n",
    "print(test_loss_hist_averaged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elt_losses = lambda idx: plot_losses_var_cutoff(idx, test_loss_hist_averaged)\n",
    "\n",
    "# Slider widget\n",
    "slider = IntSlider(min=0, max=no_cutoffs-1, value=0, description='Cutoff')\n",
    "\n",
    "# Interactive plot\n",
    "interact(plot_elt_losses, idx=slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_hist(cutoff_idx, bins, bottom_percentile, top_percentile, feature):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Feature is either a list of lists or a list of floats\n",
    "    feat_indices = find_outlier_indices(feature[:, cutoff_idx], top_percentile=top_percentile, bottom_percentile=bottom_percentile)[2]\n",
    "    plt.hist(feature[feat_indices, cutoff_idx], color='b', bins=bins)\n",
    "    # plt.title(f'Ensemble evaluations for cutoff $\\Lambda:${cutoffs[cutoff_idx]: .2f}')\n",
    "    plt.xlabel('Rosenbrock evaluation')\n",
    "    plt.ylabel('Relative frequency')\n",
    "    # plt.axhline(float(y_true), color='orange', label=f'True y: {float(y_true): .2f}')\n",
    "    # average_learnt_y = np.mean(feature[:, idx])\n",
    "    # plt.axhline(average_learnt_y, color='pink', label=f'Ensemble avg learnt y: {average_learnt_y: .2f}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/relu_act/act-relu_ensemble_eval_ts-1_cutoff-{cutoffs[cutoff_idx]}_outlier-t-{top_percentile}-b-{bottom_percentile}.pdf')\n",
    "    # plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_evals_hist = lambda cutoff_idx, bins, bottom_percentile, top_percentile: plot_loss_hist(cutoff_idx, bins, bottom_percentile, top_percentile, test_loss_hist_averaged)\n",
    "\n",
    "# Slider widget\n",
    "cutoff_slider = IntSlider(min=0, max=no_cutoffs-1, value=0, description='Cutoff')\n",
    "bins = IntSlider(min=1, max=100, value=50, description='Histogram bins')\n",
    "bottom_percentile = FloatText(min=0, max=100, value=1, description='Bottom acceptance percentile')\n",
    "top_percentile = FloatText(min=0, max=100, value=99, description='Top acceptance percentile')\n",
    "\n",
    "# Interactive plot\n",
    "interact(plot_loss_evals_hist, cutoff_idx=cutoff_slider, bins=bins, bottom_percentile=bottom_percentile, top_percentile=top_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average cutoff losses for the test samples ensemble \n",
    "*(averaged over ensemble and test samples for each ensemble member)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the averaging\n",
    "test_loss_hist_twice_averaged = np.average(test_loss_hist_averaged, axis=0)\n",
    "test_loss_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "# Feature is either a list of lists or a list of floats\n",
    "plt.scatter(cutoffs, test_loss_hist_twice_averaged, marker='o', color='b')\n",
    "# plt.title('Average test loss/cutoff')\n",
    "plt.xlabel('Cutoff')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/relu_act/average_loss_per_cutoff.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll choose just a single test evaulation (wlog the first in each ensemble member) and evaluate the flow as the cutoff is increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import MultivariateGaussianDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the test data seed is the same for a couple of ensemble members\n",
    "num_test_samples = hyperparam_list[0].num_test_samples\n",
    "means_x = hyperparam_list[0].means_x\n",
    "cov_x = hyperparam_list[0].cov_x\n",
    "test_seed = hyperparam_list[0].test_seed\n",
    "learnable_func = hyperparam_list[0].learnable_func\n",
    "\n",
    "test_x_dataset = MultivariateGaussianDataset(num_samples=num_test_samples,\n",
    "                                             means=means_x, cov=cov_x,\n",
    "                                             seed=test_seed)\n",
    "examination_data = test_x_dataset[40]\n",
    "y_true = learnable_func(examination_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test_sample_evaluation_hist = torch.tensor(evaluation_hist)[:, :, 40].numpy()\n",
    "single_test_sample_evaluation_hist = single_test_sample_evaluation_hist[mask]\n",
    "\n",
    "# Remove the poorly trained models\n",
    "single_test_sample_evaluation_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_cutoff(idx, feature):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Feature is either a list of lists or a list of floats\n",
    "    plt.scatter(np.arange(len(feature[:, idx])), feature[:, idx], marker='o', color='b')\n",
    "    plt.title(f'Ensemble evaluations for {cutoffs[idx]: .2f}')\n",
    "    plt.xlabel('Ensemble member index')\n",
    "    plt.ylabel('Test function evaluation')\n",
    "    plt.axhline(float(y_true), color='orange', label=f'True y: {float(y_true): .2f}')\n",
    "    average_learnt_y = np.mean(feature[:, idx])\n",
    "    plt.axhline(average_learnt_y, color='pink', label=f'Ensemble avg learnt y: {average_learnt_y: .2f}')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elt_evals = lambda idx: plot_eval_cutoff(idx, single_test_sample_evaluation_hist)\n",
    "\n",
    "# Slider widget\n",
    "slider = IntSlider(min=0, max=no_cutoffs-1, value=0, description='Cutoff')\n",
    "\n",
    "# Interactive plot\n",
    "interact(plot_elt_evals, idx=slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_hist(cutoff_idx, bins, bottom_percentile, top_percentile, feature):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Feature is either a list of lists or a list of floats\n",
    "    feat_indices = find_outlier_indices(feature[:, cutoff_idx], top_percentile=top_percentile, bottom_percentile=bottom_percentile)[2]\n",
    "    plt.hist(feature[feat_indices, cutoff_idx], color='b', bins=bins)\n",
    "    plt.title(f'Ensemble evaluations for cutoff $\\Lambda:${cutoffs[cutoff_idx]: .2f}')\n",
    "    plt.xlabel('Rosenbrock evaluation')\n",
    "    plt.ylabel('Relative frequency')\n",
    "    # plt.axhline(float(y_true), color='orange', label=f'True y: {float(y_true): .2f}')\n",
    "    # average_learnt_y = np.mean(feature[:, idx])\n",
    "    # plt.axhline(average_learnt_y, color='pink', label=f'Ensemble avg learnt y: {average_learnt_y: .2f}')\n",
    "    plt.grid(True)\n",
    "    # plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elt_evals = lambda cutoff_idx, bins, bottom_percentile, top_percentile: plot_eval_hist(cutoff_idx, bins, bottom_percentile, top_percentile, single_test_sample_evaluation_hist)\n",
    "\n",
    "# Slider widget\n",
    "cutoff_slider = IntSlider(min=0, max=no_cutoffs-1, value=0, description='Cutoff')\n",
    "bins = IntSlider(min=1, max=100, value=50, description='Histogram bins')\n",
    "bottom_percentile = FloatText(min=0, max=100, value=1, description='Bottom acceptance percentile')\n",
    "top_percentile = FloatText(min=0, max=100, value=99, description='Top acceptance percentile')\n",
    "\n",
    "# Interactive plot\n",
    "interact(plot_elt_evals, cutoff_idx=cutoff_slider, bins=bins, bottom_percentile=bottom_percentile, top_percentile=top_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_final_eval_hist_fit(cutoff_idx, bins, bottom_percentile, top_percentile, feature):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Feature is either a list of lists or a list of floats\n",
    "    feat_indices = find_outlier_indices(feature[:, cutoff_idx], top_percentile=top_percentile, bottom_percentile=bottom_percentile)[2]\n",
    "    hist_values, bin_edges, _ = plt.hist(feature[feat_indices, cutoff_idx], color='b', bins=bins, density=False)\n",
    "\n",
    "    # Fit a Gaussian to the histogram\n",
    "    mu, sigma = norm.fit(feature[feat_indices, cutoff_idx])\n",
    "    # Symmetric axis code\n",
    "    # get y-axis limits of the plot\n",
    "    low, high = plt.xlim()\n",
    "    # find the new limits\n",
    "    bound = max(abs(low), abs(high))\n",
    "    # set new limits\n",
    "    plt.xlim(-bound, bound)\n",
    "    x = np.linspace(-bound, bound, 5000)\n",
    "    fitted_curve = norm.pdf(x, mu, sigma) * (bin_edges[1] - bin_edges[0]) * len(feature[feat_indices, cutoff_idx])\n",
    "    plt.plot(x, fitted_curve, 'r-', color='orange', label=f'Fitted Gaussian (μ={mu:.2f}, σ={sigma:.2f})')\n",
    "    plt.axvline(y_true[0], color='pink', linewidth=3, label='True value')\n",
    "\n",
    "    # plt.title(f'Ensemble evaluations for cutoff $\\Lambda:${cutoffs[cutoff_idx]: .2f}')\n",
    "    plt.xlabel('Test function evaluation')\n",
    "    plt.ylabel('Relative frequency')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    # plt.xlim((-100, 100))\n",
    "    \n",
    "\n",
    "\n",
    "    plt.savefig(f'plots/relu_quadratic/relu_quadratic_cutoff_{cutoffs[cutoff_idx]}.pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elt_evals = lambda cutoff_idx, bins, bottom_percentile, top_percentile: plot_final_eval_hist_fit(cutoff_idx, bins, bottom_percentile, top_percentile, single_test_sample_evaluation_hist)\n",
    "\n",
    "# Slider widget\n",
    "cutoff_slider = IntSlider(min=0, max=no_cutoffs-1, value=0, description='Cutoff')\n",
    "bins = IntSlider(min=1, max=1000, value=50, description='Histogram bins')\n",
    "bottom_percentile = FloatText(min=0, max=100, value=0, description='Bottom acceptance percentile')\n",
    "top_percentile = FloatText(min=0, max=100, value=100, description='Top acceptance percentile')\n",
    "\n",
    "# Interactive plot\n",
    "interact(plot_elt_evals, cutoff_idx=cutoff_slider, bins=bins, bottom_percentile=bottom_percentile, top_percentile=top_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
